"""
Embeddings Module

This module handles the generation of text embeddings using:
- Sentence Transformers (default)
- Can be extended to support other embedding models
"""

from sentence_transformers import SentenceTransformer
import numpy as np
import torch
import logging
import os

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EmbeddingGenerator:
    """
    Class for generating embeddings from text using sentence transformers.
    """
    
    def __init__(self, model_name='all-MiniLM-L6-v2', device=None):
        """
        Initialize the embedding generator with the specified model.
        
        Args:
            model_name (str): Name of the sentence transformer model to use
            device (str, optional): Device to run the model on ('cpu' or 'cuda')
        """
        logger.info(f"Initializing EmbeddingGenerator with model: {model_name}")
        
        # Determine device (CPU/GPU)
        if device is None:
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        logger.info(f"Using device: {device}")
        
        try:
            self.model = SentenceTransformer(model_name, device=device)
            self.dimension = self.model.get_sentence_embedding_dimension()
            logger.info(f"Model loaded successfully. Embedding dimension: {self.dimension}")
        except Exception as e:
            logger.error(f"Error loading model {model_name}: {str(e)}")
            raise
    
    def generate(self, texts, batch_size=32, show_progress_bar=True):
        """
        Generate embeddings for a list of texts.
        
        Args:
            texts (str or list): Text(s) to generate embeddings for
            batch_size (int): Batch size for processing
            show_progress_bar (bool): Whether to display a progress bar
            
        Returns:
            numpy.ndarray: Array of embeddings
        """
        if isinstance(texts, str):
            texts = [texts]
        
        logger.info(f"Generating embeddings for {len(texts)} texts")
        
        try:
            embeddings = self.model.encode(
                texts,
                batch_size=batch_size,
                show_progress_bar=show_progress_bar,
                convert_to_numpy=True
            )
            
            logger.info(f"Successfully generated {len(embeddings)} embeddings")
            return embeddings
        except Exception as e:
            logger.error(f"Error generating embeddings: {str(e)}")
            raise
    
    def get_embedding_dimension(self):
        """
        Get the dimension of the embeddings generated by this model.
        
        Returns:
            int: Embedding dimension
        """
        return self.dimension
    
    def similarity(self, embedding1, embedding2):
        """
        Calculate cosine similarity between two embeddings.
        
        Args:
            embedding1 (numpy.ndarray): First embedding
            embedding2 (numpy.ndarray): Second embedding
            
        Returns:
            float: Cosine similarity score
        """
        # Ensure embeddings are normalized
        embedding1 = embedding1 / np.linalg.norm(embedding1)
        embedding2 = embedding2 / np.linalg.norm(embedding2)
        
        # Calculate cosine similarity
        return np.dot(embedding1, embedding2)
